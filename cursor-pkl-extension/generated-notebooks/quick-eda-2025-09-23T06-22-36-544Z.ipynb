{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Exploratory Data Analysis\n",
        "\n",
        "Comprehensive overview of dataset characteristics, distributions, and relationships\n",
        "\n",
        "**Generated by PKL Extension** | Success Rate: 92.0% | Est. Time: ~8min\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "plt.style.use(\"default\")\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and inspect dataset structure\n",
        "\n",
        "Expected output: Dataset dimensions and column information\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Overview",
        "df = pd.read_csv('real_data.csv')",
        "print(f\"Dataset shape: {df.shape}\")",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")",
        "df.info()"
      ],
      "metadata": {
        "step_id": 1,
        "expected_output": "Dataset dimensions and column information"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Check for missing values and duplicates\n",
        "\n",
        "Expected output: Missing value statistics and duplicate count\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Check",
        "print(\"Missing Values:\")",
        "missing_stats = df.isnull().sum()",
        "missing_pct = (missing_stats / len(df)) * 100",
        "quality_df = pd.DataFrame({",
        "    'Missing_Count': missing_stats,",
        "    'Missing_Percentage': missing_pct",
        "})",
        "print(quality_df[quality_df.Missing_Count > 0].sort_values('Missing_Count', ascending=False))",
        "",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "step_id": 2,
        "expected_output": "Missing value statistics and duplicate count"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate statistical summary\n",
        "\n",
        "Expected output: Descriptive statistics for numerical and categorical variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical Summary",
        "print(\"Numerical Variables Summary:\")",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns",
        "if len(numerical_cols) > 0:",
        "    display(df[numerical_cols].describe())",
        "",
        "print(\"\\nCategorical Variables Summary:\")",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns",
        "if len(categorical_cols) > 0:",
        "    for col in categorical_cols[:5]:  # Show first 5 categorical columns",
        "        print(f\"\\n{col}: {df[col].nunique()} unique values\")",
        "        print(df[col].value_counts().head())"
      ],
      "metadata": {
        "step_id": 3,
        "expected_output": "Descriptive statistics for numerical and categorical variables"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create distribution visualizations\n",
        "\n",
        "Expected output: Histogram plots showing distributions of numerical variables\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution Visualizations",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "",
        "# Set up the plotting style",
        "plt.style.use('default')",
        "sns.set_palette(\"husl\")",
        "",
        "# Numerical distributions",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns",
        "if len(numerical_cols) > 0:",
        "    n_cols = min(len(numerical_cols), 4)",
        "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols",
        "    ",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))",
        "    if n_rows == 1:",
        "        axes = [axes] if n_cols == 1 else axes",
        "    else:",
        "        axes = axes.flatten()",
        "    ",
        "    for i, col in enumerate(numerical_cols[:12]):  # Limit to 12 columns",
        "        df[col].hist(bins=30, ax=axes[i], alpha=0.7)",
        "        axes[i].set_title(f'Distribution of {col}')",
        "        axes[i].set_xlabel(col)",
        "        axes[i].set_ylabel('Frequency')",
        "    ",
        "    # Hide empty subplots",
        "    for i in range(len(numerical_cols), len(axes)):",
        "        axes[i].set_visible(False)",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()"
      ],
      "metadata": {
        "step_id": 4,
        "expected_output": "Histogram plots showing distributions of numerical variables"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Correlation analysis\n",
        "\n",
        "Expected output: Correlation heatmap and list of highly correlated variable pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Analysis",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns",
        "if len(numerical_cols) > 1:",
        "    correlation_matrix = df[numerical_cols].corr()",
        "    ",
        "    # Create correlation heatmap",
        "    plt.figure(figsize=(12, 8))",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', ",
        "                center=0, square=True, linewidths=0.5)",
        "    plt.title('Correlation Matrix Heatmap')",
        "    plt.tight_layout()",
        "    plt.show()",
        "    ",
        "    # Find high correlations",
        "    high_corr_pairs = []",
        "    for i in range(len(correlation_matrix.columns)):",
        "        for j in range(i+1, len(correlation_matrix.columns)):",
        "            corr_val = correlation_matrix.iloc[i, j]",
        "            if abs(corr_val) > 0.7:",
        "                high_corr_pairs.append((",
        "                    correlation_matrix.columns[i], ",
        "                    correlation_matrix.columns[j], ",
        "                    corr_val",
        "                ))",
        "    ",
        "    if high_corr_pairs:",
        "        print(\"High correlation pairs (|r| > 0.7):\")",
        "        for var1, var2, corr in high_corr_pairs:",
        "            print(f\"{var1} - {var2}: {corr:.3f}\")",
        "else:",
        "    print(\"Not enough numerical variables for correlation analysis\")"
      ],
      "metadata": {
        "step_id": 5,
        "expected_output": "Correlation heatmap and list of highly correlated variable pairs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook was generated using the \"Quick Exploratory Data Analysis\" procedure pattern.\n",
        "\n",
        "**Next steps you might consider:**\n",
        "- Review the results and identify any data quality issues\n",
        "- Adjust parameters based on your specific use case\n",
        "- Explore additional analyses based on the insights gained\n",
        "\n",
        "*Generated by PKL Extension - Procedural Knowledge Library*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "pkl_metadata": {
      "pattern_id": "quick-eda",
      "pattern_name": "Quick Exploratory Data Analysis",
      "generated_at": "2025-09-23T06:22:36.544Z",
      "parameters": {
        "dataset_path": "real_data.csv"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}