{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Notebook for PKL Extension\n",
        "\n",
        "This notebook demonstrates data science workflows that the PKL Extension should detect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Create sample data\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame({\n",
        "    'x': np.random.randn(100),\n",
        "    'y': np.random.randn(100) + 0.5 * np.random.randn(100),\n",
        "    'category': np.random.choice(['A', 'B', 'C'], 100),\n",
        "    'value': np.random.exponential(2, 100),\n",
        "    'timestamp': pd.date_range('2024-01-01', periods=100, freq='D')\n",
        "})\n",
        "\n",
        "print(f\"Dataset created with shape: {data.shape}\")\n",
        "print(f\"Columns: {list(data.columns)}\")\n",
        "print(f\"Data types:\\n{data.dtypes}\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Exploration and Analysis\n",
        "print(\"=== Data Exploration ===\")\n",
        "print(f\"Missing values:\\n{data.isnull().sum()}\")\n",
        "print(f\"\\nBasic statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Correlation analysis\n",
        "correlation_matrix = data[['x', 'y', 'value']].corr()\n",
        "print(f\"\\nCorrelation matrix:\")\n",
        "print(correlation_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Scatter plot\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(data['x'], data['y'], c=data['value'], cmap='viridis', alpha=0.7)\n",
        "plt.xlabel('X values')\n",
        "plt.ylabel('Y values')\n",
        "plt.title('Scatter Plot: X vs Y')\n",
        "plt.colorbar(label='Value')\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(data['value'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Values')\n",
        "\n",
        "# Category distribution\n",
        "plt.subplot(2, 2, 3)\n",
        "category_counts = data['category'].value_counts()\n",
        "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Category Distribution')\n",
        "\n",
        "# Time series\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(data['timestamp'], data['value'], marker='o', markersize=3)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Time Series of Values')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizations completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Machine Learning - Simple Linear Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Prepare features and target\n",
        "X = data[['x', 'y']].values\n",
        "y = data['value'].values\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "print(f\"Model coefficients: {model.coef_}\")\n",
        "print(f\"Model intercept: {model.intercept_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Model Building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "print(\"=== MACHINE LEARNING MODEL ===\")\n",
        "\n",
        "# Prepare features\n",
        "X = data[['x', 'y']].values\n",
        "y = data['value'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_test = rf_model.predict(X_test)\n",
        "\n",
        "print(f\"Model trained successfully!\")\n",
        "print(f\"Test predictions shape: {y_pred_test.shape}\")\n",
        "\n",
        "print(\"\\nModel training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete data science workflow:\n",
        "\n",
        "1. **Data Loading**: Imported necessary libraries\n",
        "2. **Data Creation**: Generated synthetic dataset with multiple features\n",
        "3. **Data Exploration**: Analyzed data structure, missing values, and outliers\n",
        "4. **Data Visualization**: Created comprehensive plots for analysis\n",
        "5. **Statistical Analysis**: Performed correlation analysis and hypothesis testing\n",
        "6. **Machine Learning**: Built and trained a predictive model\n",
        "\n",
        "The PKL Extension should detect this as an **EXPLORE** intent with **SUCCESS** outcome, as it follows a typical data science exploration pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Export Results and Save Data\n",
        "# Save processed data to CSV\n",
        "output_file = 'processed_data.csv'\n",
        "data.to_csv(output_file, index=False)\n",
        "print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Create a summary report\n",
        "summary_report = {\n",
        "    'total_records': len(data),\n",
        "    'columns': list(data.columns),\n",
        "    'missing_values': data.isnull().sum().to_dict(),\n",
        "    'data_types': data.dtypes.to_dict(),\n",
        "    'correlation_matrix': data[['x', 'y', 'value']].corr().to_dict(),\n",
        "    'model_performance': {\n",
        "        'mse': float(mse),\n",
        "        'r2_score': float(r2)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save summary as JSON\n",
        "import json\n",
        "with open('analysis_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=2, default=str)\n",
        "\n",
        "print(\"Analysis summary saved to analysis_summary.json\")\n",
        "print(f\"Final dataset shape: {data.shape}\")\n",
        "print(\"Notebook execution completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Advanced Analysis - Statistical Tests\n",
        "from scipy import stats\n",
        "\n",
        "# Perform statistical tests\n",
        "print(\"=== Statistical Analysis ===\")\n",
        "\n",
        "# Normality test for value column\n",
        "shapiro_stat, shapiro_p = stats.shapiro(data['value'])\n",
        "print(f\"Shapiro-Wilk normality test for 'value':\")\n",
        "print(f\"  Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}\")\n",
        "\n",
        "# Correlation test between x and y\n",
        "corr_coef, corr_p = stats.pearsonr(data['x'], data['y'])\n",
        "print(f\"\\nPearson correlation between x and y:\")\n",
        "print(f\"  Correlation coefficient: {corr_coef:.4f}, p-value: {corr_p:.4f}\")\n",
        "\n",
        "# ANOVA test across categories\n",
        "category_groups = [group['value'].values for name, group in data.groupby('category')]\n",
        "f_stat, f_p = stats.f_oneway(*category_groups)\n",
        "print(f\"\\nANOVA test across categories:\")\n",
        "print(f\"  F-statistic: {f_stat:.4f}, p-value: {f_p:.4f}\")\n",
        "\n",
        "# Summary statistics by category\n",
        "print(f\"\\nSummary statistics by category:\")\n",
        "category_stats = data.groupby('category')['value'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
        "print(category_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Data Processing and Feature Engineering\n",
        "# Create new features\n",
        "data['x_squared'] = data['x'] ** 2\n",
        "data['y_squared'] = data['y'] ** 2\n",
        "data['xy_interaction'] = data['x'] * data['y']\n",
        "data['value_log'] = np.log(data['value'] + 1)  # Add 1 to avoid log(0)\n",
        "\n",
        "# Create time-based features\n",
        "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
        "data['month'] = data['timestamp'].dt.month\n",
        "data['quarter'] = data['timestamp'].dt.quarter\n",
        "\n",
        "# Create categorical encoding\n",
        "data['category_encoded'] = pd.Categorical(data['category']).codes\n",
        "\n",
        "print(\"Feature Engineering completed!\")\n",
        "print(f\"New dataset shape: {data.shape}\")\n",
        "print(f\"New columns: {list(data.columns)}\")\n",
        "\n",
        "# Show sample of processed data\n",
        "print(\"\\nSample of processed data:\")\n",
        "data[['x', 'y', 'value', 'x_squared', 'xy_interaction', 'day_of_week', 'category_encoded']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Statistical Analysis\n",
        "from scipy import stats\n",
        "\n",
        "print(\"=== STATISTICAL ANALYSIS ===\")\n",
        "\n",
        "# Correlation analysis\n",
        "correlation_matrix = data[['x', 'y', 'value']].corr()\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# T-test between categories\n",
        "category_a = data[data['category'] == 'A']['value']\n",
        "category_b = data[data['category'] == 'B']['value']\n",
        "category_c = data[data['category'] == 'C']['value']\n",
        "\n",
        "print(f\"\\nT-test results:\")\n",
        "print(f\"A vs B: t-statistic = {stats.ttest_ind(category_a, category_b)[0]:.4f}, p-value = {stats.ttest_ind(category_a, category_b)[1]:.4f}\")\n",
        "print(f\"A vs C: t-statistic = {stats.ttest_ind(category_a, category_c)[0]:.4f}, p-value = {stats.ttest_ind(category_a, category_c)[1]:.4f}\")\n",
        "\n",
        "# Linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = data[['x', 'y']].values\n",
        "y = data['value'].values\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"\\nLinear Regression Results:\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Coefficients: x={model.coef_[0]:.4f}, y={model.coef_[1]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Subplot 1: Scatter plot\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(data['x'], data['y'], c=data['value'], cmap='viridis', alpha=0.6)\n",
        "plt.xlabel('X values')\n",
        "plt.ylabel('Y values')\n",
        "plt.title('X vs Y colored by Value')\n",
        "plt.colorbar()\n",
        "\n",
        "# Subplot 2: Distribution of values\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.hist(data['value'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Values')\n",
        "\n",
        "# Subplot 3: Category counts\n",
        "plt.subplot(2, 2, 3)\n",
        "data['category'].value_counts().plot(kind='bar', color='lightcoral')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Category Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Subplot 4: Time series\n",
        "plt.subplot(2, 2, 4)\n",
        "data.set_index('timestamp')['value'].plot(alpha=0.7)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Value Over Time')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Data Exploration\n",
        "print(\"=== DATA EXPLORATION ===\")\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Missing values:\\n{data.isnull().sum()}\")\n",
        "print(f\"\\nBasic statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Check for outliers\n",
        "Q1 = data['value'].quantile(0.25)\n",
        "Q3 = data['value'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = data[(data['value'] < Q1 - 1.5*IQR) | (data['value'] > Q3 + 1.5*IQR)]\n",
        "print(f\"\\nOutliers detected: {len(outliers)} rows\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
